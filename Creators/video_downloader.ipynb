{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkwYBtQdD1qy"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import subprocess\n",
        "from urllib.parse import urlparse, unquote\n",
        "from pathlib import Path\n",
        "import concurrent.futures\n",
        "\n",
        "# Define constants\n",
        "PEXELS_API_KEY = \"1wcNYkUdXROazyDGUmiAPqOW1jSGnHH5cOcR8kShhOWFhvuvOdywx9EF\"  # Replace with your actual Pexels API key\n",
        "STORAGE_DIR = \"Storage\"\n",
        "BOOKS_DIR = os.path.join(STORAGE_DIR, \"temp_texts\")\n",
        "VIDEOS_DIR = os.path.join(STORAGE_DIR, \"downloaded_videos\")\n",
        "DOWNLOAD_LOG = os.path.join(STORAGE_DIR, \"video_download_log.json\")\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(VIDEOS_DIR, exist_ok=True)\n",
        "os.makedirs(BOOKS_DIR, exist_ok=True)\n",
        "\n",
        "def get_search_terms_from_books():\n",
        "    \"\"\"Extract search terms from all book files\"\"\"\n",
        "    search_terms_by_book = {}\n",
        "    try:\n",
        "        for filename in os.listdir(BOOKS_DIR):\n",
        "            if filename.startswith('book_') and filename.endswith('.json'):\n",
        "                file_path = os.path.join(BOOKS_DIR, filename)\n",
        "                with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                    book_data = json.load(f)\n",
        "                book_num = int(filename.split('_')[1].split('.')[0])\n",
        "                if 'pexels_videos' in book_data:\n",
        "                    # Create a list of search terms from the pexels_videos array\n",
        "                    search_terms = [video_data['search_term'] for video_data in book_data['pexels_videos']]\n",
        "                    search_terms_by_book[book_num] = {\n",
        "                        'title': book_data['title'],\n",
        "                        'search_terms': search_terms\n",
        "                    }\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading book files: {e}\")\n",
        "    return search_terms_by_book\n",
        "\n",
        "def search_pexels_videos(query, per_page=5, orientation=\"landscape\"):\n",
        "    \"\"\"Search for videos on Pexels API with the given query\"\"\"\n",
        "    url = \"https://api.pexels.com/videos/search\"\n",
        "    params = {\n",
        "        \"query\": query,\n",
        "        \"per_page\": per_page,\n",
        "        \"orientation\": orientation,\n",
        "        \"size\": \"large\"  # Prefer large (high quality) videos\n",
        "    }\n",
        "    headers = {\"Authorization\": PEXELS_API_KEY}\n",
        "    try:\n",
        "        response = requests.get(url, params=params, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        results = response.json()\n",
        "        return results.get(\"videos\", [])\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error searching Pexels API for '{query}': {e}\")\n",
        "        return []\n",
        "\n",
        "def get_best_video_file(video_data):\n",
        "    \"\"\"Select the highest quality video file that's reasonable to download\"\"\"\n",
        "    if not video_data or \"video_files\" not in video_data:\n",
        "        return None\n",
        "    video_files = video_data[\"video_files\"]\n",
        "    preferred_files = []\n",
        "    for file in video_files:\n",
        "        if \"height\" not in file:\n",
        "            continue\n",
        "        if file[\"height\"] >= 720:\n",
        "            if \"file_type\" in file and file[\"file_type\"].startswith(\"video/\"):\n",
        "                preferred_files.append(file)\n",
        "    if not preferred_files and video_files:\n",
        "        preferred_files = [file for file in video_files if \"file_type\" in file and file[\"file_type\"].startswith(\"video/\")]\n",
        "    preferred_files.sort(key=lambda x: x.get(\"height\", 0), reverse=True)\n",
        "    return preferred_files[0] if preferred_files else None\n",
        "\n",
        "def download_video_segment(url, output_path, duration=10):\n",
        "    \"\"\"\n",
        "    Download only the first `duration` seconds of a video using ffmpeg.\n",
        "    This assumes ffmpeg is installed and available in your PATH.\n",
        "    \"\"\"\n",
        "    command = [\n",
        "        \"ffmpeg\",\n",
        "        \"-y\",                   # Overwrite output file if it exists\n",
        "        \"-i\", url,              # Input URL\n",
        "        \"-t\", str(duration),    # Duration (in seconds)\n",
        "        \"-c\", \"copy\",           # Copy codec to avoid re-encoding\n",
        "        output_path\n",
        "    ]\n",
        "    try:\n",
        "        subprocess.run(command, capture_output=True, check=True)\n",
        "        return True\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error downloading video segment: {e.stderr.decode()}\")\n",
        "        return False\n",
        "\n",
        "def get_file_extension_from_url(url):\n",
        "    \"\"\"Extract file extension from URL\"\"\"\n",
        "    parsed_url = urlparse(url)\n",
        "    path = unquote(parsed_url.path)\n",
        "    ext = os.path.splitext(path)[1]\n",
        "    if not ext:\n",
        "        ext = \".mp4\"\n",
        "    return ext\n",
        "\n",
        "def load_download_log():\n",
        "    \"\"\"Load the download log to avoid re-downloading videos\"\"\"\n",
        "    if os.path.exists(DOWNLOAD_LOG):\n",
        "        try:\n",
        "            with open(DOWNLOAD_LOG, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            return {\"downloaded_videos\": {}}\n",
        "    return {\"downloaded_videos\": {}}\n",
        "\n",
        "def save_download_log(log_data):\n",
        "    \"\"\"Save the download log\"\"\"\n",
        "    with open(DOWNLOAD_LOG, 'w', encoding='utf-8') as f:\n",
        "        json.dump(log_data, f, indent=4)\n",
        "\n",
        "def download_videos_for_books(max_videos_per_book=3, max_concurrent_downloads=3):\n",
        "    \"\"\"Download videos for all books with search terms\"\"\"\n",
        "    search_terms_by_book = get_search_terms_from_books()\n",
        "    download_log = load_download_log()\n",
        "    if \"downloaded_videos\" not in download_log:\n",
        "        download_log[\"downloaded_videos\"] = {}\n",
        "\n",
        "    # Maintain a per-book counter for tasks to ensure unique filenames\n",
        "    tasks_counter = {}\n",
        "\n",
        "    download_tasks = []\n",
        "    for book_num, book_data in search_terms_by_book.items():\n",
        "        book_str = str(book_num)\n",
        "        # Create a subfolder for each book\n",
        "        book_folder = os.path.join(VIDEOS_DIR, f\"book_{book_num}\")\n",
        "        os.makedirs(book_folder, exist_ok=True)\n",
        "\n",
        "        # Initialize counter from log if available, else start at 0\n",
        "        tasks_counter[book_num] = len(download_log[\"downloaded_videos\"].get(book_str, []))\n",
        "\n",
        "        # If already reached max videos (as per log), skip\n",
        "        if tasks_counter[book_num] >= max_videos_per_book:\n",
        "            print(f\"Already have {max_videos_per_book} videos for book {book_num}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        for search_term in book_data[\"search_terms\"]:\n",
        "            # Check counter before queuing another task\n",
        "            if tasks_counter[book_num] >= max_videos_per_book:\n",
        "                break\n",
        "            print(f\"Searching for videos with term: '{search_term}' for book {book_num}\")\n",
        "            videos = search_pexels_videos(search_term)\n",
        "            random.shuffle(videos)\n",
        "            for video in videos:\n",
        "                if tasks_counter[book_num] >= max_videos_per_book:\n",
        "                    break\n",
        "                video_file = get_best_video_file(video)\n",
        "                if video_file and \"link\" in video_file:\n",
        "                    video_url = video_file[\"link\"]\n",
        "                    # Skip if this URL has already been downloaded\n",
        "                    if any(log.get(\"url\") == video_url for logs in download_log[\"downloaded_videos\"].values() for log in logs):\n",
        "                        print(\"Video URL already downloaded. Skipping.\")\n",
        "                        continue\n",
        "                    ext = get_file_extension_from_url(video_url)\n",
        "                    # Use a naming scheme: vid_1, vid_2, etc.\n",
        "                    tasks_counter[book_num] += 1\n",
        "                    vid_index = tasks_counter[book_num]\n",
        "                    output_filename = f\"vid_{vid_index}{ext}\"\n",
        "                    output_path = os.path.join(book_folder, output_filename)\n",
        "                    download_tasks.append({\n",
        "                        \"url\": video_url,\n",
        "                        \"output_path\": output_path,\n",
        "                        \"book_num\": book_num,\n",
        "                        \"video_id\": video.get(\"id\", \"unknown\"),\n",
        "                        \"video_url\": video.get(\"url\", \"\"),\n",
        "                        \"width\": video_file.get(\"width\", 0),\n",
        "                        \"height\": video_file.get(\"height\", 0),\n",
        "                        \"search_term\": search_term\n",
        "                    })\n",
        "                    # Stop after finding one suitable video per search term\n",
        "                    break\n",
        "\n",
        "    if download_tasks:\n",
        "        print(f\"Found {len(download_tasks)} videos to download.\")\n",
        "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_concurrent_downloads) as executor:\n",
        "            future_to_task = {}\n",
        "            for task in download_tasks:\n",
        "                future = executor.submit(\n",
        "                    download_video_segment,\n",
        "                    task[\"url\"],\n",
        "                    task[\"output_path\"],\n",
        "                    10  # duration in seconds\n",
        "                )\n",
        "                future_to_task[future] = task\n",
        "            for future in concurrent.futures.as_completed(future_to_task):\n",
        "                task = future_to_task[future]\n",
        "                book_num_str = str(task[\"book_num\"])\n",
        "                try:\n",
        "                    success = future.result()\n",
        "                    if success:\n",
        "                        print(f\"Successfully downloaded video for book {task['book_num']}\")\n",
        "                        if book_num_str not in download_log[\"downloaded_videos\"]:\n",
        "                            download_log[\"downloaded_videos\"][book_num_str] = []\n",
        "                        download_log[\"downloaded_videos\"][book_num_str].append({\n",
        "                            \"filename\": os.path.basename(task[\"output_path\"]),\n",
        "                            \"url\": task[\"url\"],\n",
        "                            \"video_id\": task[\"video_id\"],\n",
        "                            \"pexels_url\": task[\"video_url\"],\n",
        "                            \"width\": task[\"width\"],\n",
        "                            \"height\": task[\"height\"],\n",
        "                            \"search_term\": task[\"search_term\"],\n",
        "                            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "                        })\n",
        "                        save_download_log(download_log)\n",
        "                    else:\n",
        "                        print(f\"Failed to download video for book {task['book_num']}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing download for book {task['book_num']}: {e}\")\n",
        "    else:\n",
        "        print(\"No new videos to download.\")\n",
        "    return download_log\n",
        "\n",
        "def main():\n",
        "    print(\"ðŸŽ¬ PEXELS VIDEO DOWNLOADER ðŸŽ¬\")\n",
        "    print(\"Downloading first 10-second segments for book recommendations...\")\n",
        "    # Download up to 5 videos per book\n",
        "    download_log = download_videos_for_books(max_videos_per_book=5)\n",
        "    total_videos = sum(len(videos) for videos in download_log[\"downloaded_videos\"].values())\n",
        "    print(f\"\\nSummary: Downloaded {total_videos} videos in total\")\n",
        "    for book_num, videos in download_log[\"downloaded_videos\"].items():\n",
        "        if videos:\n",
        "            print(f\"Book {book_num}: {len(videos)} videos\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}